# -*- coding: utf-8 -*-
"""PandemicLLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/140bfJCejrAi2KFx3lWvCw6u2-0HRHx6r
"""

!pip install easydict

# âœ… Install dependencies
!pip install pandas transformers accelerate easydict matplotlib

# âœ… Imports
import pandas as pd
import torch
import random
from torch.utils.data import Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments
from easydict import EasyDict

# âœ… Load dataset
data = pd.read_pickle("/content/processed_v5_4.pkl")

# Extract the actual DataFrame from the EasyDict
if isinstance(data, EasyDict):
    if 'data' in data and isinstance(data['data'], pd.DataFrame):
        df = data['data'].copy()
    else:
        for key, value in data.items():
            if isinstance(value, pd.DataFrame):
                df = value.copy()
                break
        else:
            raise ValueError("Could not find DataFrame in EasyDict")
else:
    df = data.copy()

# Now map your actual columns to the expected ones
if 'Trend_prompt' in df.columns:
    df['prompt'] = df['Trend_prompt']
elif 'dynamic_prompt' in df.columns:
    df['prompt'] = df['dynamic_prompt']
else:
    raise ValueError("Could not find suitable prompt column. Available columns: " + str(df.columns.tolist()))

if 'Abs_Trend' in df.columns:
    # First inspect the Abs_Trend values
    print("Sample Abs_Trend values:", df['Abs_Trend'].head())

    # Handle case where Abs_Trend contains lists
    def map_trend(trend_value):
        if isinstance(trend_value, list):
            # Take the first element if it's a list
            trend_value = trend_value[0] if len(trend_value) > 0 else 'stable'

        # Convert to string and lowercase for consistent mapping
        trend_str = str(trend_value).lower()

        trend_map = {
            'decreasing': 'Moderate Decrease',
            'stable': 'Stable',
            'increasing': 'Moderate Increase',
            'substantial decrease': 'Substantial Decrease',
            'moderate decrease': 'Moderate Decrease',
            'moderate increase': 'Moderate Increase',
            'substantial increase': 'Substantial Increase'
        }
        return trend_map.get(trend_str, 'Stable')  # Default to Stable if not found

    df['target'] = df['Abs_Trend'].apply(map_trend)
else:
    raise ValueError("Could not find suitable target column. Available columns: " + str(df.columns.tolist()))

if 'State' in df.columns:
    df['state'] = df['State']
elif 'state_name' in df.columns:
    df['state'] = df['state_name']
else:
    raise ValueError("Could not find suitable state column. Available columns: " + str(df.columns.tolist()))

print("Loaded dataset with shape:", df.shape)
print("Sample prompt:", df['prompt'].iloc[0])
print("Sample target:", df['target'].iloc[0])
print("Sample state:", df['state'].iloc[0])

# âœ… Add wastewater surveillance summary
def generate_wastewater_summary():
    trend = random.choice(["increasing", "stable", "decreasing"])
    return f"The recent wastewater viral load trend is {trend}, with moderate concentration last week."

df["wastewater_summary"] = df.apply(lambda row: generate_wastewater_summary(), axis=1)
df["full_prompt"] = df["prompt"] + "\n% Wastewater Surveillance\n" + df["wastewater_summary"]

# âœ… Add emoji forecast summary
emoji_map = {
    "Substantial Decrease": "ðŸ“‰",
    "Moderate Decrease": "â¬‡ï¸",
    "Stable": "âž¡ï¸",
    "Moderate Increase": "â¬†ï¸",
    "Substantial Increase": "ðŸ“ˆ"
}
df["emoji_forecast"] = df["target"].map(emoji_map)

# âœ… (Optional) Region filtering - check state names match exactly
region_states = ["California", "Washington", "Oregon", "Nevada"]  # Try full state names
region_df = df[df["state"].isin(region_states)].reset_index(drop=True)
print("Using region-specific data:", region_df.shape)

# âœ… Custom Dataset
class PandemicDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_len=512):
        self.prompts = dataframe["full_prompt"].tolist()
        self.targets = dataframe["target"].tolist()
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.label_map = {
            "Substantial Decrease": 0,
            "Moderate Decrease": 1,
            "Stable": 2,
            "Moderate Increase": 3,
            "Substantial Increase": 4
        }

    def __len__(self):
        return len(self.prompts)

    def __getitem__(self, idx):
        enc = self.tokenizer(self.prompts[idx], truncation=True, padding="max_length", max_length=self.max_len, return_tensors="pt")
        return {
            "input_ids": enc["input_ids"].squeeze(),
            "attention_mask": enc["attention_mask"].squeeze(),
            "labels": torch.tensor(self.label_map[self.targets[idx]])
        }

# Consider using a smaller model if you run into memory issues
model_name = "tiiuae/falcon-7b-instruct"  # Or try "tiiuae/falcon-1b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)

# âœ… Create train/val split
dataset = PandemicDataset(region_df, tokenizer)
train_size = int(0.8 * len(dataset))
train_data = torch.utils.data.Subset(dataset, range(train_size))
val_data = torch.utils.data.Subset(dataset, range(train_size, len(dataset)))

# âœ… Training arguments (updated parameter name)
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=1,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    warmup_steps=10,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
    eval_strategy="epoch",  # Changed from evaluation_strategy
    save_strategy="epoch",
    save_total_limit=1,
    fp16=True
)

# âœ… Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=val_data
)

# âœ… Train
trainer.train()

# âœ… Save final model
model.save_pretrained("/content/fine_tuned_pandemic_llm")
tokenizer.save_pretrained("/content/fine_tuned_pandemic_llm")

!free -h

# âœ… Install dependencies
!pip install -q pandas transformers accelerate bitsandbytes matplotlib
!pip install -q huggingface_hub

# âœ… Login to Hugging Face (required for Falcon models)
from huggingface_hub import notebook_login
notebook_login()

# âœ… Imports
import pandas as pd
import torch
import random
from torch.utils.data import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    BitsAndBytesConfig,
    DataCollatorForLanguageModeling
)
from easydict import EasyDict

# âœ… Load dataset
data = pd.read_pickle("/content/processed_v5_4.pkl")

# Extract the actual DataFrame from the EasyDict
if isinstance(data, EasyDict):
    if 'data' in data and isinstance(data['data'], pd.DataFrame):
        df = data['data'].copy()
    else:
        for key, value in data.items():
            if isinstance(value, pd.DataFrame):
                df = value.copy()
                break
        else:
            raise ValueError("Could not find DataFrame in EasyDict")
else:
    df = data.copy()

# Now map your actual columns to the expected ones
if 'Trend_prompt' in df.columns:
    df['prompt'] = df['Trend_prompt']
elif 'dynamic_prompt' in df.columns:
    df['prompt'] = df['dynamic_prompt']
else:
    raise ValueError("Could not find suitable prompt column. Available columns: " + str(df.columns.tolist()))

# Convert targets to text labels
if 'Abs_Trend' in df.columns:
    def map_trend(trend_value):
        if isinstance(trend_value, list):
            trend_value = trend_value[0] if len(trend_value) > 0 else 'stable'

        trend_str = str(trend_value).lower()

        trend_map = {
            'decreasing': 'DECREASING TREND',
            'stable': 'STABLE TREND',
            'increasing': 'INCREASING TREND',
            'substantial decrease': 'SUBSTANTIAL DECREASE',
            'moderate decrease': 'MODERATE DECREASE',
            'moderate increase': 'MODERATE INCREASE',
            'substantial increase': 'SUBSTANTIAL INCREASE'
        }
        return trend_map.get(trend_str, 'STABLE TREND')

    df['target_text'] = df['Abs_Trend'].apply(map_trend)
else:
    raise ValueError("Could not find suitable target column. Available columns: " + str(df.columns.tolist()))

if 'State' in df.columns:
    df['state'] = df['State']
elif 'state_name' in df.columns:
    df['state'] = df['state_name']
else:
    raise ValueError("Could not find suitable state column. Available columns: " + str(df.columns.tolist()))

print("Loaded dataset with shape:", df.shape)
print("Sample prompt:", df['prompt'].iloc[0])
print("Sample target:", df['target_text'].iloc[0])
print("Sample state:", df['state'].iloc[0])

# âœ… Add wastewater surveillance summary
def generate_wastewater_summary():
    trend = random.choice(["increasing", "stable", "decreasing"])
    return f"The recent wastewater viral load trend is {trend}, with moderate concentration last week."

df["wastewater_summary"] = df.apply(lambda row: generate_wastewater_summary(), axis=1)

# Combine prompt and target for text generation
df["full_text"] = df["prompt"] + "\n% Wastewater Surveillance\n" + df["wastewater_summary"] + "\nForecast: " + df["target_text"]

# âœ… (Optional) Region filtering - using full state names
region_states = ["California", "Washington", "Oregon", "Nevada"]
region_df = df[df["state"].isin(region_states)].reset_index(drop=True)
print("Using region-specific data:", region_df.shape)

# âœ… Custom Dataset for Text Generation
class TextGenerationDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_len=256):
        self.texts = dataframe["full_text"].tolist()
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encodings = self.tokenizer(
            self.texts[idx],
            truncation=True,
            max_length=self.max_len,
            padding="max_length",
            return_tensors="pt"
        )
        return {
            'input_ids': encodings['input_ids'].flatten(),
            'attention_mask': encodings['attention_mask'].flatten(),
            'labels': encodings['input_ids'].flatten()  # For causal LM, labels are same as input_ids
        }

# âœ… Load model and tokenizer with 4-bit quantization
try:
    # First try Falcon-1B (requires authentication)
    model_name = "tiiuae/falcon-1b-instruct"
    print("Attempting to load Falcon-1B...")

    # Configure 4-bit quantization
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )

    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        trust_remote_code=True,
        quantization_config=bnb_config,
        device_map="auto"
    )
except Exception as e:
    print(f"Failed to load Falcon-1B: {str(e)}")
    print("Falling back to GPT-2 model...")

    # Fallback to GPT-2 if Falcon fails
    model_name = "gpt2"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(model_name)
    model.resize_token_embeddings(len(tokenizer))

# Set pad token if not already set
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# âœ… Create train/val split
dataset = TextGenerationDataset(region_df, tokenizer)
train_size = int(0.8 * len(dataset))
train_data = torch.utils.data.Subset(dataset, range(train_size))
val_data = torch.utils.data.Subset(dataset, range(train_size, len(dataset)))

# âœ… Training arguments with memory optimizations
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=1,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=4,
    gradient_checkpointing=True,
    fp16=True,
    warmup_steps=10,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
    eval_strategy="epoch",  # Changed from evaluation_strategy
    save_strategy="epoch",
    save_total_limit=1,
    report_to="none",
    learning_rate=5e-5
)

# âœ… Use proper data collator for language modeling
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False  # Causal language modeling
)

# âœ… Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=val_data,
    data_collator=data_collator
)

# âœ… Train
print("Starting training...")
trainer.train()

# âœ… Save final model
print("Saving model...")
model.save_pretrained("/content/fine_tuned_pandemic_llm")
tokenizer.save_pretrained("/content/fine_tuned_pandemic_llm")
print("Training complete and model saved!")

# âœ… Install dependencies
!pip install -q pandas transformers accelerate bitsandbytes matplotlib wordcloud scikit-learn seaborn
!pip install -q huggingface_hub

# âœ… Imports
import pandas as pd
import torch
import random
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix
import seaborn as sns
from torch.utils.data import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    BitsAndBytesConfig,
    DataCollatorForLanguageModeling
)
from easydict import EasyDict
from huggingface_hub import notebook_login

# âœ… Login to Hugging Face (required for Falcon models)
notebook_login()

# âœ… Load dataset
data = pd.read_pickle("/content/processed_v5_4.pkl")

# Extract the actual DataFrame from the EasyDict
if isinstance(data, EasyDict):
    if 'data' in data and isinstance(data['data'], pd.DataFrame):
        df = data['data'].copy()
    else:
        for key, value in data.items():
            if isinstance(value, pd.DataFrame):
                df = value.copy()
                break
        else:
            raise ValueError("Could not find DataFrame in EasyDict")
else:
    df = data.copy()

# Now map your actual columns to the expected ones
if 'Trend_prompt' in df.columns:
    df['prompt'] = df['Trend_prompt']
elif 'dynamic_prompt' in df.columns:
    df['prompt'] = df['dynamic_prompt']
else:
    raise ValueError("Could not find suitable prompt column. Available columns: " + str(df.columns.tolist()))

# Convert targets to text labels
if 'Abs_Trend' in df.columns:
    def map_trend(trend_value):
        if isinstance(trend_value, list):
            trend_value = trend_value[0] if len(trend_value) > 0 else 'stable'

        trend_str = str(trend_value).lower()

        trend_map = {
            'decreasing': 'DECREASING TREND',
            'stable': 'STABLE TREND',
            'increasing': 'INCREASING TREND',
            'substantial decrease': 'SUBSTANTIAL DECREASE',
            'moderate decrease': 'MODERATE DECREASE',
            'moderate increase': 'MODERATE INCREASE',
            'substantial increase': 'SUBSTANTIAL INCREASE'
        }
        return trend_map.get(trend_str, 'STABLE TREND')

    df['target_text'] = df['Abs_Trend'].apply(map_trend)
else:
    raise ValueError("Could not find suitable target column. Available columns: " + str(df.columns.tolist()))

if 'State' in df.columns:
    df['state'] = df['State']
elif 'state_name' in df.columns:
    df['state'] = df['state_name']
else:
    raise ValueError("Could not find suitable state column. Available columns: " + str(df.columns.tolist()))

print("Loaded dataset with shape:", df.shape)
print("Sample prompt:", df['prompt'].iloc[0])
print("Sample target:", df['target_text'].iloc[0])
print("Sample state:", df['state'].iloc[0])

# âœ… Add wastewater surveillance summary
def generate_wastewater_summary():
    trend = random.choice(["increasing", "stable", "decreasing"])
    return f"The recent wastewater viral load trend is {trend}, with moderate concentration last week."

df["wastewater_summary"] = df.apply(lambda row: generate_wastewater_summary(), axis=1)

# Combine prompt and target for text generation
df["full_text"] = df["prompt"] + "\n% Wastewater Surveillance\n" + df["wastewater_summary"] + "\nForecast: " + df["target_text"]

# âœ… (Optional) Region filtering - using full state names
region_states = ["California", "Washington", "Oregon", "Nevada"]
region_df = df[df["state"].isin(region_states)].reset_index(drop=True)
print("Using region-specific data:", region_df.shape)

# âœ… Custom Dataset for Text Generation
class TextGenerationDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_len=256):
        self.texts = dataframe["full_text"].tolist()
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encodings = self.tokenizer(
            self.texts[idx],
            truncation=True,
            max_length=self.max_len,
            padding="max_length",
            return_tensors="pt"
        )
        return {
            'input_ids': encodings['input_ids'].flatten(),
            'attention_mask': encodings['attention_mask'].flatten(),
            'labels': encodings['input_ids'].flatten()
        }

# âœ… Load model and tokenizer with 4-bit quantization
try:
    # First try Falcon-1B (requires authentication)
    model_name = "tiiuae/falcon-1b-instruct"
    print("Attempting to load Falcon-1B...")

    # Configure 4-bit quantization
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )

    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        trust_remote_code=True,
        quantization_config=bnb_config,
        device_map="auto"
    )
except Exception as e:
    print(f"Failed to load Falcon-1B: {str(e)}")
    print("Falling back to GPT-2 model...")

    # Fallback to GPT-2 if Falcon fails
    model_name = "gpt2"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(model_name)
    model.resize_token_embeddings(len(tokenizer))

# Set pad token if not already set
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# âœ… Create train/val split
dataset = TextGenerationDataset(region_df, tokenizer)
train_size = int(0.8 * len(dataset))
train_data = torch.utils.data.Subset(dataset, range(train_size))
val_data = torch.utils.data.Subset(dataset, range(train_size, len(dataset)))

# âœ… Training arguments with memory optimizations
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=1,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=4,
    gradient_checkpointing=True,
    fp16=True,
    warmup_steps=10,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
    eval_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=1,
    report_to="none",
    learning_rate=5e-5
)

# âœ… Use proper data collator for language modeling
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# âœ… Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=val_data,
    data_collator=data_collator
)

# âœ… Train
print("Starting training...")
trainer.train()

# âœ… Save final model
print("Saving model...")
model.save_pretrained("/content/fine_tuned_pandemic_llm")
tokenizer.save_pretrained("/content/fine_tuned_pandemic_llm")
print("Training complete and model saved!")

# ======================================
# VISUALIZATION SECTION
# ======================================

# 1. Plot training loss
print("\nPlotting training metrics...")
history = trainer.state.log_history
train_loss = [log['loss'] for log in history if 'loss' in log]
eval_loss = [log['eval_loss'] for log in history if 'eval_loss' in log]

plt.figure(figsize=(10, 5))
plt.plot(train_loss, label='Training Loss')
plt.plot(eval_loss, label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Steps')
plt.ylabel('Loss')
plt.legend()
plt.savefig('/content/training_loss.png', bbox_inches='tight')
plt.show()

# 2. Target distribution
print("\nPlotting target distribution...")
plt.figure(figsize=(10, 5))
region_df['target_text'].value_counts().plot(kind='bar')
plt.title('Distribution of Target Classes')
plt.xlabel('Trend Categories')
plt.ylabel('Count')
plt.savefig('/content/target_distribution.png', bbox_inches='tight')
plt.show()

# 3. State-wise distribution
print("\nPlotting state distribution...")
plt.figure(figsize=(12, 6))
region_df['state'].value_counts().plot(kind='bar')
plt.title('Distribution by State')
plt.xlabel('States')
plt.ylabel('Count')
plt.savefig('/content/state_distribution.png', bbox_inches='tight')
plt.show()

# 4. Word cloud
print("\nGenerating word cloud...")
all_text = " ".join(region_df['full_text'].tolist())
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)

plt.figure(figsize=(15, 8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.savefig('/content/wordcloud.png', bbox_inches='tight')
plt.show()

# 5. Sample predictions
print("\nGenerating sample predictions...")
sample_texts = region_df['full_text'].sample(3).tolist()

# Move model to CUDA if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

for i, text in enumerate(sample_texts):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=256)

    # Move input tensors to the same device as the model
    inputs = {k: v.to(device) for k, v in inputs.items()}

    outputs = model.generate(
        input_ids=inputs['input_ids'],
        attention_mask=inputs['attention_mask'],
        max_length=256,
        temperature=0.7,
        do_sample=True  # Added to properly use temperature
    )

    print(f"\nSample {i+1}:")
    print("Input:")
    print(text)
    print("\nGenerated Output:")
    print(tokenizer.decode(outputs[0], skip_special_tokens=True))
    print("-"*80)

# 6. Feature importance
print("\nAnalyzing important words...")
vectorizer = TfidfVectorizer(max_features=50)
X_vec = vectorizer.fit_transform(region_df['full_text'])
feature_names = vectorizer.get_feature_names_out()
dense = X_vec.todense()
mean_tfidf = dense.mean(axis=0).tolist()[0]

plt.figure(figsize=(12, 8))
plt.barh(feature_names, mean_tfidf)
plt.title('Most Important Words (TF-IDF)')
plt.xlabel('Average TF-IDF Score')
plt.savefig('/content/feature_importance.png', bbox_inches='tight')
plt.show()

# 7. Confusion Matrix (simplified)
print("\nGenerating confusion matrix...")
# Get a subset for faster evaluation
eval_subset = region_df.sample(min(100, len(region_df)))

predictions = []
for text in eval_subset['full_text']:
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=256)
    inputs = {k: v.to(device) for k, v in inputs.items()}  # Move to device

    outputs = model.generate(
        input_ids=inputs['input_ids'],
        attention_mask=inputs['attention_mask'],
        max_length=256,
        temperature=0.7,
        do_sample=True
    )
    pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    predictions.append(pred_text)

# Simplified trend extraction - adjust based on your actual output
pred_trends = []
for pred in predictions:
    if "DECREASING" in pred: pred_trends.append("DECREASING TREND")
    elif "INCREASING" in pred: pred_trends.append("INCREASING TREND")
    else: pred_trends.append("STABLE TREND")

cm = confusion_matrix(eval_subset['target_text'], pred_trends,
                     labels=["DECREASING TREND", "STABLE TREND", "INCREASING TREND"])

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=["DECREASING", "STABLE", "INCREASING"],
            yticklabels=["DECREASING", "STABLE", "INCREASING"])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.savefig('/content/confusion_matrix.png', bbox_inches='tight')
plt.show()

print("\nAll visualizations completed and saved to /content/")